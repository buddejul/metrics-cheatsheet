\section{Maximum Likelihood Estimation}
\textbf{Def 6.1 (likelihood function)}: $L_n(\theta) = \prod_{i=1}^nf(\textbf{x}_i;\theta)$.\\
Equivalently, we define the \textit{log-likelihood function} as $\log(L_n(\theta))$.\\
\textbf{Thm 6.1}: Suppose $\textbf{X}$ is a random vector with pdf or pmf $f(\textbf{x};\theta_0)$. Then $E(\log(f(\textbf{x};\theta)))\geq E(ln(f(\textbf{X};\theta))), \forall \theta in \Theta$.\\
\textbf{Thm 6.2}: For $\tau(\theta)$ and $\hat{\theta}_n$ MLE of $\theta$, we have $\tau(\hat{\theta}_n)$ is MLE of $\tau(\theta)$.

\subsection{Distribution of the MLE}
\textbf{MLE Limit Distribution}: $\sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow{d} N(0, A^{-^1}BA^{-1})$ with $A = E_\theta[-\frac{\partial^2}{\partial\theta\partial'\theta}ln(f(\textbf{X}_i);\theta)]$ and $B = E_\theta[\frac{\partial}{\partial\theta}ln(f(\textbf{X}_i;\theta)\frac{\partial\theta}{\partial\theta'})ln(f(\textbf{X}_i;\theta))]$. B \textit{var-cov} matrix of the score (since score mean zero by FOC). A is \textit{Fisher information}.\\
\textbf{Thm 6.3}: Under weak reg. cond. (diff; interch. integr./diff.) we have $A=B$.

\subsection{CRLB}
We could try to define "best" estimator in terms of MSE. However, MSE might depend on $\theta$ (e.g. $\bar{X}_n$ vs. $1$, the latter dominates for $\theta = 1$).\\
Progress: Focus on \textit{unbiased} estimators and thus variance.\\
\textbf{Thm 6.4}: $\{X_i\}$ rs from $f(\textbf{x};\theta)$, $\hat{\theta}_n$ estimator of $\theta$. Then under some reg conds $Var_\theta[\hat{\theta}_n] = \frac{(\frac{\partial}{\partial\theta}E_\theta[\hat{\theta}_n])^2}{nE_\theta[(\frac{\partial}{\partial\theta}\log(f(\textbf{X};\theta)))^2]}$.\\
With unbiased estimators (numerator equals one) estimators attaining the lower bound are called \textit{efficient}.\\
Caveats: (1) Finite-sample efficient estimators rare; even MLE often biased, (2) allowing some bias can reduce variance and thus MSE, (3) MSE might not be criterion of interest.\\
\textbf{Relative efficiency}: $E_\theta[(\hat{\theta}_{1,n}-\theta)^2] \leq E_\theta[(\hat{\theta}_{2,n}-\theta)^2]$ for all $\theta\in\Theta$ and strict for some.\\
\textbf{Asymptotic efficiency}: Asymptotic distribution often implies \textit{asymptotically unbiased}, efficiency than means attaining CRLB asymptotically. Thus, the MLE is asymptotically efficient. Similar, for two estimators (possibly not attaining the CRLB) we can say one is \textit{asymptotically relatively more efficient} (i.e. has lower asymptotic variance).